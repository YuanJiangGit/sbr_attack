# -*- coding: utf-8 -*-
# @Author  : Jiang Yuan
# @Time    : 2021/6/4 19:36
# @Function: evaluate the adversarial examples generated by each attack methods.
import os
import pandas as pd
from numpy import mean
from tqdm import notebook as tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
import tensorflow as tf
import tensorflow_hub as hub
import torch
import math

from htps.constrains.LanguageTool import LanguageTool

PYTORCH_DEVICE = 0
TF_DEVICE = 0
# torch.cuda.set_device(0)


class GPT2Metric:
    '''
    GPT2Metric measures the percent difference is perplexities of original text  ùë•  and adversarial example  ùë•ùëéùëëùë£ .
    '''
    def __init__(self):
        self._model = AutoModelForCausalLM.from_pretrained("gpt2")
        # self._model.to(device=f'cuda:{PYTORCH_DEVICE}') # jiangyuan
        self._tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)

    def perplexity(self, text):
        input_ids = self._tokenizer.encode(text)
        input_ids = input_ids[: self._tokenizer.model_max_length - 2]
        input_ids.insert(0, self._tokenizer.bos_token_id)
        input_ids.append(self._tokenizer.eos_token_id)
        input_ids = torch.tensor(input_ids)
        # input_ids = input_ids.to(device=f'cuda:{PYTORCH_DEVICE}') # jiangyuan
        with torch.no_grad():
            loss = self._model(input_ids, labels=input_ids)[0].item()

        perplexity = math.exp(loss)
        return perplexity

    def calc_metric(self, orig_text, new_text):
        orig_perplexity = self.perplexity(orig_text)
        new_perplexity = self.perplexity(new_text)
        return orig_perplexity, new_perplexity, (new_perplexity - orig_perplexity) / orig_perplexity


class USEMetric:
    '''
    USEMetric measures the Universal Sentence Encoder similarity between  ùë•  and  ùë•ùëéùëëùë£ .
    '''
    def __init__(self):
        # tfhub_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
        tfhub_url = "D:\\downloads\\universal-sentence-encoder_4"
        with tf.device(f'/device:CPU:{TF_DEVICE}'):
            self._model = hub.load(tfhub_url)

    def encode(self, orig_text, new_text):
        with tf.device(f'/device:CPU:{TF_DEVICE}'):
            return self._model([orig_text, new_text]).numpy()

    def get_angular_sim(self, emb1, emb2):
        cos_sim = torch.nn.CosineSimilarity(dim=0)(emb1, emb2)
        return 1 - (torch.acos(cos_sim) / math.pi)

    def calc_metric(self, orig_text, new_text):
        orig_emb, new_emb = self.encode(orig_text, new_text)
        orig_emb = torch.tensor(orig_emb)
        new_emb = torch.tensor(new_emb)
        sim = self.get_angular_sim(orig_emb, new_emb).item()
        return sim


class PercentageOfWordsChanged:
    '''
    PercentageOfWordsChanged: measures the percentage of words swapped in  ùë•  to produce  ùë•ùëéùëëùë£ .
    '''
    def calc_metric(self, orig_text, new_text):
        # orig_words = set(orig_text.split())
        # new_words = set(new_text.split())
        # words_changed = new_words - (orig_words & new_words)
        # words_changed_num = 0
        # for one in new_words:
        #     if one in words_changed:
        #         words_changed_num += 1
        # return words_changed_num * 100 / len(orig_words)
        words_changed_num = 0
        orig_words = orig_text.split()
        new_words = new_text.split()
        for i in range(min(len(orig_words), len(new_words))):
            if orig_words[i] != new_words[i]:
                words_changed_num += 1
        return words_changed_num


class Evaluator:
    '''
    Evaluator: evaluator runs all three metrics for each sample and reports the average.
    '''
    def __init__(self):
        self.use_metric = USEMetric()
        self.gpt2_metric = GPT2Metric()
        self.percentageOfWordsChanged = PercentageOfWordsChanged()
        self.languageTool = LanguageTool()

    def evaluate_sim(self, df):
        attack_total_sim = []
        for i, row in df.iterrows():
            original_text = row["original_text"].replace("[", "").replace("]", "")
            perturbed_text = row["perturbed_text"].replace("[", "").replace("]", "")
            attack_sim = self.use_metric.calc_metric(original_text, perturbed_text)
            attack_total_sim.append(attack_sim)
        return attack_total_sim

    def evaluate_ppl(self, df):
        total_origin_pp_diff = []
        total_attack_pp_diff = []
        attack_more_diff = []
        for i, row in df.iterrows():
            original_text = row["original_text"].replace("[", "").replace("]", "")
            perturbed_text = row["perturbed_text"].replace("[", "").replace("]", "")
            origin_pp_diff, attack_pp_diff, attack_diff_pert = self.gpt2_metric.calc_metric(original_text,
                                                                                            perturbed_text)
            total_origin_pp_diff.append(origin_pp_diff)
            total_attack_pp_diff.append(attack_pp_diff)
            attack_more_diff.append(attack_diff_pert)
        return total_origin_pp_diff, total_attack_pp_diff, attack_more_diff

    def evaluate_word_num(self, df):
        total_insert_num = []
        total_swap_num = []
        for i, row in df.iterrows():
            total_insert_num.append(row["insert_num"])
            total_swap_num.append(row["swap_num"])
        return total_insert_num, total_swap_num

    def evaluate_grammer_error(self, df):
        grammer_error_diff = []
        for i, row in df.iterrows():
            original_text = row["original_text"].replace("[", "").replace("]", "")
            perturbed_text = row["perturbed_text"].replace("[", "").replace("]", "")
            origin_grammer_error = self.languageTool.get_errors(original_text)
            perturb_grammer_error = self.languageTool.get_errors(perturbed_text)
            # if perturb_grammer_error - origin_grammer_error > 5:
            #     print(i)
            if origin_grammer_error != perturb_grammer_error:
                grammer_error_diff.append(perturb_grammer_error-origin_grammer_error)
        return grammer_error_diff, len(df)

    def evaluate_two_class_info(self, df):
        pos_skip = len(df.loc[(df['ground_truth_output'] == 1) & (df['original_output'] == 0)])
        neg_skip = len(df.loc[(df['ground_truth_output'] == 0) & (df['original_output'] == 1)])
        pos = len(df.loc[(df['ground_truth_output'] == 1) & (df['original_output'] == 1)])
        neg = len(df.loc[(df['ground_truth_output'] == 0) & (df['original_output'] == 0)])
        pos_succ = len(df.loc[(df['ground_truth_output'] == 1) & (df['result_type'] == 'Successful')])
        pos_fail = pos - pos_succ
        neg_succ = len(df.loc[(df['ground_truth_output'] == 0) & (df['result_type'] == 'Successful')])
        neg_fail = neg - neg_succ
        return pos, neg, pos_skip, neg_skip, pos_succ, pos_fail, neg_succ, neg_fail


def result_file(dir, train_info):
    files=os.listdir(dir)
    for file in files:
        tag=True
        for train in train_info:
            if train not in file:
                tag=False
        if tag:
            return file

def main():
    evaluator = Evaluator()
    # projects = ['ambari', 'camel', 'derby', 'chromium']
    projects = ['ambari']#, 'camel', 'derby', 'chromium']
    # attack_models = ['PWWSRen2019', 'TextBuggerLi2018', 'TextFoolerJin2019', 'DeepWordBugGao2018']
    # ['PWWSRen2019', 'TextBuggerLi2018', 'TextFoolerJin2019', 'DeepWordBugGao2018']
    attack_models = ['HTPsAttackV1']#, 'HTPsAttackV2', 'HTPsAttackV3', 'HTPsAttackV4']
    models = ["LTRWES"]
    houzhuis = ["wordnet", "embedding", "hownet"]
    # houzhuis = ["fun2+width1", "fun3+width1", "fun1+width8", "fun2+width8", "fun3+width8"]
    # houzhuis = ['Â≠óÁ¨¶Á∫ß', 'Âè™ÊèíÂÖ•', 'Âè™ÊõøÊç¢']
    # , 'embed+cos', 'hownet+cos', 'wordnet+cos', 'hownetembed']
    # houzhuis = ["Êó†ËØçÂµåÂÖ•", "ÊúâËØçÂµåÂÖ•"]#, "Êó†ËØçÊÄß", "Êó†Áõ∏‰ººÂ∫¶"]
    # houzhuis = ["Âè™ÊèíÂÖ•"]
    # RESULT_ROOT_DIR = "../resources/attack_results/Áé∞ÊúâÊñπÊ≥ï"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/ÊõøÊç¢Á≠ñÁï•/FARSECÊó†ÊèíÂÖ•"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/Êâ∞Âä®Á∫¶Êùü"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/ËØÑÂàÜÂáΩÊï∞"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/ÂêØÂèëÂºè"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/Êâ∞Âä®Á≠ñÁï•"
    # RESULT_ROOT_DIR = "../resources/attack_results/final-htpsattack/Êó†Á∫¶Êùü"
    # RESULT_ROOT_DIR = "../resources/attack_results/train-attack"
    # RESULT_ROOT_DIR = "../resources/attack_results/defense-attack"
    RESULT_ROOT_DIR = "../resources/attack_results/Ê†∑‰æã/Êñ∞"
    # RESULT_ROOT_DIR = "../resources/attack_results/final-htpsattack/final/test"
    # RESULT_ROOT_DIR = "../resources/attack_results/train-attack/epoch1"
    num_files = len(projects) * len(attack_models) * len(models) * len(houzhuis)
    pbar = tqdm.tqdm(total=num_files, smoothing=0)
    for project in projects:
        for attack_model in attack_models:
            for model in models:
                for houzhui in houzhuis:
                    print("=" * 45)
                    print(f"{model}---{project}---{attack_model}---{houzhui}")
                    print("-" * 45)
                    file_name = result_file(RESULT_ROOT_DIR, [model, project, attack_model, houzhui, '.csv'])
                    csv_path = f"{RESULT_ROOT_DIR}/{file_name}"
                    print(csv_path)
                    df = pd.read_csv(csv_path)
                    # pos, neg, pos_skip, neg_skip, \
                    # pos_succ, pos_fail, neg_succ, neg_fail = evaluator.evaluate_two_class_info(df)
                    # print(pos, neg, pos_skip, neg_skip, pos_succ, pos_fail, neg_succ, neg_fail)
                    df = df[df['result_type'] == "Successful"]
                    attack_total_sim = evaluator.evaluate_sim(df)
                    # df['sim_score'] = attack_total_sim
                    origin_pp_diff, attack_pp_diff, attack_more_diff = evaluator.evaluate_ppl(df)
                    # df['origin_pp_diff'] = origin_pp_diff
                    # df['attack_pp_diff'] = attack_pp_diff
                    # df['attack_more_diff'] = attack_more_diff
                    # csv_path = f"{RESULT_ROOT_DIR}/temp_{attack_model}_{project}_{model}_{houzhui}.csv"
                    # df.to_csv(csv_path, encoding='utf-8')
                    # insert_num, swap_num = evaluator.evaluate_word_num(df)
                    # grammer_error_diff, length = evaluator.evaluate_word_num(df)
                    # print(grammer_error_diff)
                    # print(sum(grammer_error_diff), length, sum(grammer_error_diff) / length)
                    print(
                        f"Attack USE Sim: {round(mean(attack_total_sim), 3)} \n"
                        f"Origin PP Diff: {round(mean(origin_pp_diff), 1)} \n"
                        f"Attack PP Diff: {round(mean(attack_pp_diff), 1)} \n"
                        f"attack_more_diff: {round(mean(attack_more_diff), 3)} \n"
                        # f"grammer_error_diff: {round(sum(grammer_error_diff) / length, 5)} \n"
                        # f"avg_swap: {round(mean(swap_num), 2)} \n"
                        # f"avg_insert: {round(mean(insert_num), 2)} \n"
                    )
                    pbar.update(1)


def test_main():
    evaluator = Evaluator()
    RESULT_ROOT_DIR = "../resources/attack_results/ÂàÜÊûê/hownetembed"
    file_name = 'sim-ppl.csv'
    csv_path = f"{RESULT_ROOT_DIR}/{file_name}"
    df = pd.read_csv(csv_path)
    attack_total_sim = []
    for i, row in df.iterrows():
        original_text = row["origin"]
        perturbed_text = row["curr"]
        attack_sim = evaluator.use_metric.calc_metric(original_text, perturbed_text)
        attack_total_sim.append(attack_sim)
    df['sim_score'] = attack_total_sim
    total_origin_pp_diff = []
    total_attack_pp_diff = []
    attack_more_diff = []
    for i, row in df.iterrows():
        original_text = row["origin"]
        perturbed_text = row["curr"]
        origin_pp_diff, attack_pp_diff, attack_diff_pert = evaluator.gpt2_metric.calc_metric(original_text,
                                                                                             perturbed_text)
        total_origin_pp_diff.append(origin_pp_diff)
        total_attack_pp_diff.append(attack_pp_diff)
        attack_more_diff.append(attack_diff_pert)
    df['origin_pp_diff'] = total_origin_pp_diff
    df['attack_pp_diff'] = total_attack_pp_diff
    df['attack_more_diff'] = attack_more_diff
    csv_path = f"{RESULT_ROOT_DIR}/temp_{file_name}"
    df.to_csv(csv_path, encoding='utf-8')

if __name__ == '__main__':
    tf.enable_eager_execution(
        config=None,
        device_policy=None,
        execution_mode=None
    )
    main()
    # test_main()
