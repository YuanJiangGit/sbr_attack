# -*- coding: utf-8 -*-
# @Author  : Jiang Yuan
# @Time    : 2021/6/4 19:36
# @Function: evaluate the adversarial examples generated by each attack methods.
import os
import pandas as pd
from numpy import mean
from tqdm import notebook as tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
import tensorflow as tf
import tensorflow_hub as hub
import torch
import math

from htps.constrains.LanguageTool import LanguageTool

PYTORCH_DEVICE = 0
TF_DEVICE = 0
# torch.cuda.set_device(0)


class GPT2Metric:
    '''
    GPT2Metric measures the percent difference is perplexities of original text  𝑥  and adversarial example  𝑥𝑎𝑑𝑣 .
    '''
    def __init__(self):
        self._model = AutoModelForCausalLM.from_pretrained("gpt2")
        # self._model.to(device=f'cuda:{PYTORCH_DEVICE}') # jiangyuan
        self._tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)

    def perplexity(self, text):
        input_ids = self._tokenizer.encode(text)
        input_ids = input_ids[: self._tokenizer.model_max_length - 2]
        input_ids.insert(0, self._tokenizer.bos_token_id)
        input_ids.append(self._tokenizer.eos_token_id)
        input_ids = torch.tensor(input_ids)
        # input_ids = input_ids.to(device=f'cuda:{PYTORCH_DEVICE}') # jiangyuan
        with torch.no_grad():
            loss = self._model(input_ids, labels=input_ids)[0].item()

        perplexity = math.exp(loss)
        return perplexity

    def calc_metric(self, orig_text, new_text):
        orig_perplexity = self.perplexity(orig_text)
        new_perplexity = self.perplexity(new_text)
        return orig_perplexity, new_perplexity, (new_perplexity - orig_perplexity) / orig_perplexity


class USEMetric:
    '''
    USEMetric measures the Universal Sentence Encoder similarity between  𝑥  and  𝑥𝑎𝑑𝑣 .
    '''
    def __init__(self):
        # tfhub_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
        tfhub_url = "D:\\downloads\\universal-sentence-encoder_4"
        with tf.device(f'/device:CPU:{TF_DEVICE}'):
            self._model = hub.load(tfhub_url)

    def encode(self, orig_text, new_text):
        with tf.device(f'/device:CPU:{TF_DEVICE}'):
            return self._model([orig_text, new_text]).numpy()

    def get_angular_sim(self, emb1, emb2):
        cos_sim = torch.nn.CosineSimilarity(dim=0)(emb1, emb2)
        return 1 - (torch.acos(cos_sim) / math.pi)

    def calc_metric(self, orig_text, new_text):
        orig_emb, new_emb = self.encode(orig_text, new_text)
        orig_emb = torch.tensor(orig_emb)
        new_emb = torch.tensor(new_emb)
        sim = self.get_angular_sim(orig_emb, new_emb).item()
        return sim


class PercentageOfWordsChanged:
    '''
    PercentageOfWordsChanged: measures the percentage of words swapped in  𝑥  to produce  𝑥𝑎𝑑𝑣 .
    '''
    def calc_metric(self, orig_text, new_text):
        # orig_words = set(orig_text.split())
        # new_words = set(new_text.split())
        # words_changed = new_words - (orig_words & new_words)
        # words_changed_num = 0
        # for one in new_words:
        #     if one in words_changed:
        #         words_changed_num += 1
        # return words_changed_num * 100 / len(orig_words)
        words_changed_num = 0
        orig_words = orig_text.split()
        new_words = new_text.split()
        for i in range(min(len(orig_words), len(new_words))):
            if orig_words[i] != new_words[i]:
                words_changed_num += 1
        return words_changed_num


class Evaluator:
    '''
    Evaluator: evaluator runs all three metrics for each sample and reports the average.
    '''
    def __init__(self):
        self.use_metric = USEMetric()
        self.gpt2_metric = GPT2Metric()
        self.percentageOfWordsChanged = PercentageOfWordsChanged()
        self.languageTool = LanguageTool()

    def evaluate_sim(self, df):
        attack_total_sim = []
        for i, row in df.iterrows():
            original_text = row["original_text"].replace("[", "").replace("]", "")
            perturbed_text = row["perturbed_text"].replace("[", "").replace("]", "")
            attack_sim = self.use_metric.calc_metric(original_text, perturbed_text)
            attack_total_sim.append(attack_sim)
        return attack_total_sim

    def evaluate_ppl(self, df):
        total_origin_pp_diff = []
        total_attack_pp_diff = []
        attack_more_diff = []
        for i, row in df.iterrows():
            original_text = row["original_text"].replace("[", "").replace("]", "")
            perturbed_text = row["perturbed_text"].replace("[", "").replace("]", "")
            origin_pp_diff, attack_pp_diff, attack_diff_pert = self.gpt2_metric.calc_metric(original_text,
                                                                                            perturbed_text)
            total_origin_pp_diff.append(origin_pp_diff)
            total_attack_pp_diff.append(attack_pp_diff)
            attack_more_diff.append(attack_diff_pert)
        return total_origin_pp_diff, total_attack_pp_diff, attack_more_diff

    def evaluate_word_num(self, df):
        total_insert_num = []
        total_swap_num = []
        for i, row in df.iterrows():
            total_insert_num.append(row["insert_num"])
            total_swap_num.append(row["swap_num"])
        return total_insert_num, total_swap_num

    def evaluate_grammer_error(self, df):
        grammer_error_diff = []
        for i, row in df.iterrows():
            original_text = row["original_text"].replace("[", "").replace("]", "")
            perturbed_text = row["perturbed_text"].replace("[", "").replace("]", "")
            origin_grammer_error = self.languageTool.get_errors(original_text)
            perturb_grammer_error = self.languageTool.get_errors(perturbed_text)
            # if perturb_grammer_error - origin_grammer_error > 5:
            #     print(i)
            if origin_grammer_error != perturb_grammer_error:
                grammer_error_diff.append(perturb_grammer_error-origin_grammer_error)
        return grammer_error_diff, len(df)

    def evaluate_two_class_info(self, df):
        pos_skip = len(df.loc[(df['ground_truth_output'] == 1) & (df['original_output'] == 0)])
        neg_skip = len(df.loc[(df['ground_truth_output'] == 0) & (df['original_output'] == 1)])
        pos = len(df.loc[(df['ground_truth_output'] == 1) & (df['original_output'] == 1)])
        neg = len(df.loc[(df['ground_truth_output'] == 0) & (df['original_output'] == 0)])
        pos_succ = len(df.loc[(df['ground_truth_output'] == 1) & (df['result_type'] == 'Successful')])
        pos_fail = pos - pos_succ
        neg_succ = len(df.loc[(df['ground_truth_output'] == 0) & (df['result_type'] == 'Successful')])
        neg_fail = neg - neg_succ
        return pos, neg, pos_skip, neg_skip, pos_succ, pos_fail, neg_succ, neg_fail


def result_file(dir, train_info):
    files=os.listdir(dir)
    for file in files:
        tag=True
        for train in train_info:
            if train not in file:
                tag=False
        if tag:
            return file

def main():
    evaluator = Evaluator()
    # projects = ['ambari', 'camel', 'derby', 'chromium']
    projects = ['ambari']#, 'camel', 'derby', 'chromium']
    # attack_models = ['PWWSRen2019', 'TextBuggerLi2018', 'TextFoolerJin2019', 'DeepWordBugGao2018']
    # ['PWWSRen2019', 'TextBuggerLi2018', 'TextFoolerJin2019', 'DeepWordBugGao2018']
    attack_models = ['HTPsAttackV1']#, 'HTPsAttackV2', 'HTPsAttackV3', 'HTPsAttackV4']
    models = ["LTRWES"]
    houzhuis = ["wordnet", "embedding", "hownet"]
    # houzhuis = ["fun2+width1", "fun3+width1", "fun1+width8", "fun2+width8", "fun3+width8"]
    # houzhuis = ['字符级', '只插入', '只替换']
    # , 'embed+cos', 'hownet+cos', 'wordnet+cos', 'hownetembed']
    # houzhuis = ["无词嵌入", "有词嵌入"]#, "无词性", "无相似度"]
    # houzhuis = ["只插入"]
    # RESULT_ROOT_DIR = "../resources/attack_results/现有方法"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/替换策略/FARSEC无插入"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/扰动约束"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/评分函数"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/启发式"
    # RESULT_ROOT_DIR = "../resources/attack_results/2022/扰动策略"
    # RESULT_ROOT_DIR = "../resources/attack_results/final-htpsattack/无约束"
    # RESULT_ROOT_DIR = "../resources/attack_results/train-attack"
    # RESULT_ROOT_DIR = "../resources/attack_results/defense-attack"
    RESULT_ROOT_DIR = "../resources/attack_results/样例/新"
    # RESULT_ROOT_DIR = "../resources/attack_results/final-htpsattack/final/test"
    # RESULT_ROOT_DIR = "../resources/attack_results/train-attack/epoch1"
    num_files = len(projects) * len(attack_models) * len(models) * len(houzhuis)
    pbar = tqdm.tqdm(total=num_files, smoothing=0)
    for project in projects:
        for attack_model in attack_models:
            for model in models:
                for houzhui in houzhuis:
                    print("=" * 45)
                    print(f"{model}---{project}---{attack_model}---{houzhui}")
                    print("-" * 45)
                    file_name = result_file(RESULT_ROOT_DIR, [model, project, attack_model, houzhui, '.csv'])
                    csv_path = f"{RESULT_ROOT_DIR}/{file_name}"
                    print(csv_path)
                    df = pd.read_csv(csv_path)
                    # pos, neg, pos_skip, neg_skip, \
                    # pos_succ, pos_fail, neg_succ, neg_fail = evaluator.evaluate_two_class_info(df)
                    # print(pos, neg, pos_skip, neg_skip, pos_succ, pos_fail, neg_succ, neg_fail)
                    df = df[df['result_type'] == "Successful"]
                    attack_total_sim = evaluator.evaluate_sim(df)
                    # df['sim_score'] = attack_total_sim
                    origin_pp_diff, attack_pp_diff, attack_more_diff = evaluator.evaluate_ppl(df)
                    # df['origin_pp_diff'] = origin_pp_diff
                    # df['attack_pp_diff'] = attack_pp_diff
                    # df['attack_more_diff'] = attack_more_diff
                    # csv_path = f"{RESULT_ROOT_DIR}/temp_{attack_model}_{project}_{model}_{houzhui}.csv"
                    # df.to_csv(csv_path, encoding='utf-8')
                    # insert_num, swap_num = evaluator.evaluate_word_num(df)
                    # grammer_error_diff, length = evaluator.evaluate_word_num(df)
                    # print(grammer_error_diff)
                    # print(sum(grammer_error_diff), length, sum(grammer_error_diff) / length)
                    print(
                        f"Attack USE Sim: {round(mean(attack_total_sim), 3)} \n"
                        f"Origin PP Diff: {round(mean(origin_pp_diff), 1)} \n"
                        f"Attack PP Diff: {round(mean(attack_pp_diff), 1)} \n"
                        f"attack_more_diff: {round(mean(attack_more_diff), 3)} \n"
                        # f"grammer_error_diff: {round(sum(grammer_error_diff) / length, 5)} \n"
                        # f"avg_swap: {round(mean(swap_num), 2)} \n"
                        # f"avg_insert: {round(mean(insert_num), 2)} \n"
                    )
                    pbar.update(1)


def test_main():
    evaluator = Evaluator()
    RESULT_ROOT_DIR = "../resources/attack_results/分析/hownetembed"
    file_name = 'sim-ppl.csv'
    csv_path = f"{RESULT_ROOT_DIR}/{file_name}"
    df = pd.read_csv(csv_path)
    attack_total_sim = []
    for i, row in df.iterrows():
        original_text = row["origin"]
        perturbed_text = row["curr"]
        attack_sim = evaluator.use_metric.calc_metric(original_text, perturbed_text)
        attack_total_sim.append(attack_sim)
    df['sim_score'] = attack_total_sim
    total_origin_pp_diff = []
    total_attack_pp_diff = []
    attack_more_diff = []
    for i, row in df.iterrows():
        original_text = row["origin"]
        perturbed_text = row["curr"]
        origin_pp_diff, attack_pp_diff, attack_diff_pert = evaluator.gpt2_metric.calc_metric(original_text,
                                                                                             perturbed_text)
        total_origin_pp_diff.append(origin_pp_diff)
        total_attack_pp_diff.append(attack_pp_diff)
        attack_more_diff.append(attack_diff_pert)
    df['origin_pp_diff'] = total_origin_pp_diff
    df['attack_pp_diff'] = total_attack_pp_diff
    df['attack_more_diff'] = attack_more_diff
    csv_path = f"{RESULT_ROOT_DIR}/temp_{file_name}"
    df.to_csv(csv_path, encoding='utf-8')

if __name__ == '__main__':
    tf.enable_eager_execution(
        config=None,
        device_policy=None,
        execution_mode=None
    )
    main()
    # test_main()
